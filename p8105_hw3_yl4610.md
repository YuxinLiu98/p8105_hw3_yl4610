p8105_hw3_yl4610
================
Yuxin Liu
2022-10-08

``` r
library(tidyverse)
library(p8105.datasets)
```

# question 1

``` r
data("instacart") 
janitor::clean_names(instacart)
```

    ## # A tibble: 1,384,617 × 15
    ##    order_id product_id add_to_…¹ reord…² user_id eval_…³ order…⁴ order…⁵ order…⁶
    ##       <int>      <int>     <int>   <int>   <int> <chr>     <int>   <int>   <int>
    ##  1        1      49302         1       1  112108 train         4       4      10
    ##  2        1      11109         2       1  112108 train         4       4      10
    ##  3        1      10246         3       0  112108 train         4       4      10
    ##  4        1      49683         4       0  112108 train         4       4      10
    ##  5        1      43633         5       1  112108 train         4       4      10
    ##  6        1      13176         6       0  112108 train         4       4      10
    ##  7        1      47209         7       0  112108 train         4       4      10
    ##  8        1      22035         8       1  112108 train         4       4      10
    ##  9       36      39612         1       0   79431 train        23       6      18
    ## 10       36      19660         2       1   79431 train        23       6      18
    ## # … with 1,384,607 more rows, 6 more variables: days_since_prior_order <int>,
    ## #   product_name <chr>, aisle_id <int>, department_id <int>, aisle <chr>,
    ## #   department <chr>, and abbreviated variable names ¹​add_to_cart_order,
    ## #   ²​reordered, ³​eval_set, ⁴​order_number, ⁵​order_dow, ⁶​order_hour_of_day

I loaded the libraries and data from the p8105.datasets. Then I used
janitor to clean the names. This dataset contains variables such as
order_id, product_id, add_to_cart_order, reordered, user_id, eval_set,
order_number, order_dow, order_hour_of_day, days_since_prior_order,
product_name, aisle_id, department_id, aisle, department. It contains 15
columns and 1384617 rows, with each row representing a single product
from an instacart order. Variables include identifiers for user, order,
and product; the order in which each product was added to the cart.
There are several order-level variables, describing the day and time of
the order, and number of days since prior order. Then there are several
item-specific variables, describing the product name (e.g. Yogurt,
Avocado), department (e.g. dairy and eggs, produce), and aisle
(e.g. yogurt, fresh fruits), and whether the item has been ordered by
this user in the past. In total, there are 39123 products found in
131209 orders from 131209 distinct users.

``` r
instacart =
instacart %>%
  group_by(aisle) %>%
  summarize(n_obs = n())%>%
  arrange(desc(n_obs)) %>%
  filter(n_obs > 10000) %>% 
  ggplot(aes(x=aisle, y = n_obs)) + geom_point() 
```

There are 134 aisles and fresh vegetables are the most items ordered
from. I used group_by and summarize to look at the number of aisles and
then used arrange to arrange the number from largest to smallest. I used
filter to limit number of items to aisles with more than 10000 items
ordered by. I used ggplot to make a scatterplot to show the number of
items ordered in each aisle.

Make a plot that shows the number of items ordered in each aisle,
limiting this to aisles with more than 10000 items ordered. Arrange
aisles sensibly, and organize your plot so others can read it. Make a
table showing the three most popular items in each of the aisles “baking
ingredients”, “dog food care”, and “packaged vegetables fruits”. Include
the number of times each item is ordered in your table. Make a table
showing the mean hour of the day at which Pink Lady Apples and Coffee
Ice Cream are ordered on each day of the week; format this table for
human readers (i.e. produce a 2 x 7 table).

# question 2

``` r
accel_data = read_csv( "./dataset/accel_data.csv") %>%
janitor::clean_names() %>% 
mutate (
  weekday_vs_weekend = ifelse(day %in% c("Saturday", "Sunday"), 0, 1),
  weekday_vs_weekend = as.factor (weekday_vs_weekend)
  ) %>%
  pivot_longer(
    activity_1:activity_1440,
    names_to = "activity",
    names_prefix = "activity_", 
    values_to = "activity_counts") 
```

    ## Rows: 35 Columns: 1443
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr    (1): day
    ## dbl (1442): week, day_id, activity.1, activity.2, activity.3, activity.4, ac...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
accel_data 
```

    ## # A tibble: 50,400 × 6
    ##     week day_id day    weekday_vs_weekend activity activity_counts
    ##    <dbl>  <dbl> <chr>  <fct>              <chr>              <dbl>
    ##  1     1      1 Friday 1                  1                   88.4
    ##  2     1      1 Friday 1                  2                   82.2
    ##  3     1      1 Friday 1                  3                   64.4
    ##  4     1      1 Friday 1                  4                   70.0
    ##  5     1      1 Friday 1                  5                   75.0
    ##  6     1      1 Friday 1                  6                   66.3
    ##  7     1      1 Friday 1                  7                   53.8
    ##  8     1      1 Friday 1                  8                   47.8
    ##  9     1      1 Friday 1                  9                   55.5
    ## 10     1      1 Friday 1                  10                  43.0
    ## # … with 50,390 more rows

I used read_csv to read dataset and used janitor to clean the names of
data. Then I used mutate to create the new variable weekday_vs_weekend
by using ifelse. If day equals to Saturday or Sunday, weekday_vs_weekend
equals to 0; If day equals to weekdays, weekday_vs_weekend equals to 1.
Then I used mutate to convert them to factors. I used pivot_longer to
pivot the categories of activities into one column and pivot
corresponding values to activity_counts. I also used names_to to dropped
the prefix activity\_ to make it cleaner.

This dataset contains variables such as week, day_id, day,
weekday_vs_weekend, activity, activity_counts. It contains 6 columns and
50400 rows. Thus, there are 50400 observations.

``` r
accel_data %>%
  group_by (week, day) %>%
  summarise (
  activity_total = sum(activity_counts)) %>%
  arrange(desc(activity_total)) %>%
  pivot_wider (
    names_from = "day",
    values_from = "activity_total")%>% 
  select (Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday) %>% 
  knitr::kable() 
```

    ## `summarise()` has grouped output by 'week'. You can override using the
    ## `.groups` argument.
    ## Adding missing grouping variables: `week`

| week |    Monday |  Tuesday | Wednesday | Thursday |   Friday | Saturday | Sunday |
|-----:|----------:|---------:|----------:|---------:|---------:|---------:|-------:|
|    3 | 685910.00 | 381507.0 |    468869 | 371230.0 | 467420.0 |   382928 | 467052 |
|    1 |  78828.07 | 307094.2 |    340115 | 355923.6 | 480542.6 |   376254 | 631105 |
|    5 | 389080.00 | 367824.0 |    445366 | 549658.0 | 620860.0 |     1440 | 138421 |
|    2 | 295431.00 | 423245.0 |    440962 | 474048.0 | 568839.0 |   607175 | 422018 |
|    4 | 409450.00 | 319568.0 |    434460 | 340291.0 | 154049.0 |     1440 | 260617 |

mutate( day = forcats::fct_relevel(day, c(“Monday”, “Tuesday”,
“Wednesday”, “Thursday”, “Friday”, “Saturday”, “Sunday”)))

I used group_by to make groupings explicit so that they can be included
in subsequent operations. Then I used summarise to create a total
activity variable for each day by aggregating across minutes via sum
function. I used arrange to rank the activity_total from highest to
lowest. I used knitr::kable to create a table in rmd file.

Are any trends apparent?

``` r
accel_data %>%
  ggplot(aes(x = activity, y = activity_counts, color = day)) + 
  geom_line(alpha = 0.5) + 
    labs( 
    title = "24-hour activity time courses for each day",
    x = "minute",
    y = "day") 
```

![](p8105_hw3_yl4610_files/figure-gfm/unnamed-chunk-6-1.png)<!-- --> I
used ggplot to create a scatterplot to show the 24-hour activity time
courses for each day. I used color = day to indicate day of the week. I
used labs to add title, x-axis label, and y-axis label. Describe in
words any patterns or conclusions you can make based on this graph.

# question3

``` r
data("ny_noaa") 
ny_noaadf = 
ny_noaa %>%
janitor::clean_names() %>% 
  mutate (tmax = as.integer(tmax),
          tmin = as.integer(tmin)) %>% 
separate(date, into = c("year", "month", "day"), sep = "-") %>% 
  mutate (
      year = as.integer(year),
      month = as.integer(month),
      day = as.integer(day)) %>% 
  mutate (
    prcp = prcp/10,
      tmax = tmax/10,
        tmin = tmin/10)
```

I loaded ny_noaa dataset from the p8105.datasets library. This dataset
contains variables such as id, date, prcp, snow, snwd, tmax, tmin. It
contains 7 columns and 2595176 rows. Thus, there are 2595176
observations.

missing data is an issue

``` r
ny_noaadf %>% 
  group_by(snow) %>% 
    summarize(
    n_obs = n()) %>% 
arrange(desc(n_obs))
```

    ## # A tibble: 282 × 2
    ##     snow   n_obs
    ##    <int>   <int>
    ##  1     0 2008508
    ##  2    NA  381221
    ##  3    25   31022
    ##  4    13   23095
    ##  5    51   18274
    ##  6    76   10173
    ##  7     8    9962
    ##  8     5    9748
    ##  9    38    9197
    ## 10     3    8790
    ## # … with 272 more rows

The most commonly observed values for snow is 0.

``` r
ny_plot1 = 
ny_noaadf %>%
mutate(month = month.name[month]) %>% 
filter(month %in% c("January","July")) %>% 
group_by(id, year, month) %>% 
summarize(
average_max_temp = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(
  aes(x = year, y = average_max_temp, color=average_max_temp, group=id)) +
  geom_line(alpha = 0.5, size = .1) +
  facet_grid(.~month)
```

    ## `summarise()` has grouped output by 'id', 'year'. You can override using the
    ## `.groups` argument.

``` r
ny_plot1
```

    ## Warning: Removed 5640 row(s) containing missing values (geom_path).

![](p8105_hw3_yl4610_files/figure-gfm/unnamed-chunk-9-1.png)<!-- --> Is
there any observable / interpretable structure? Any outliers?

``` r
ny_ploti =
  ny_noaadf %>%
  ggplot(
  aes(x = tmin, y = tmax)) +
  geom_line(alpha = 0.5, size = .1)
ny_ploti
```

    ## Warning: Removed 1134420 row(s) containing missing values (geom_path).

![](p8105_hw3_yl4610_files/figure-gfm/unnamed-chunk-10-1.png)<!-- -->

``` r
ny_plotii =
  ny_noaadf %>%
  filter(snow>0 & snow<100) %>% 
  ggplot(aes(x = snow, fill = year)) +
  geom_density(alpha = .4, adjust = .5, color = "blue")
ny_plotii
```

![](p8105_hw3_yl4610_files/figure-gfm/unnamed-chunk-11-1.png)<!-- -->
